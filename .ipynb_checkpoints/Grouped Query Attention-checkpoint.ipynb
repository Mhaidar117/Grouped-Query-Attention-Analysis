{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Query Attention\n",
    "\n",
    "### In this notebook we will compare Grouped Query Attetion with two othe forms of handling attention heads.\n",
    "\n",
    "Authored by: Michael Haidar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped-Querry Attention provides three major advantages over other architectures such as Multi-Headed Attetion and Multi-Querry Attetion:\n",
    "  ##### - Efficiency: \n",
    "    - GQA strikes a balance between flexibility and efficiency. By reducing the number of key-value pairs, GQA      reduces memory consumption and computation cost, which is crucial for large-scale models.\n",
    "   #####  - Parameter Sharing:\n",
    "    - Shared key-value pairs within groups still allow some diversity in the attention patterns, especially since the query projections remain unique for each head. This means that GQA can capture useful relationships without needing as many parameters as MHA.\n",
    "   #####  - Scalability:\n",
    "    - For very large models, where the number of heads is high (e.g., 64 or 128 heads in transformer-based large language models), GQA provides a scalable solution that reduces memory and computational demands while still maintaining adequate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import tracemalloc\n",
    "class style():\n",
    "  RED = '\\033[31m'\n",
    "  GREEN = '\\033[32m'\n",
    "  BLUE = '\\033[34m'\n",
    "  RESET = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "batch_size = 8\n",
    "seq_len = 5\n",
    "d_model = 16  # Hidden size of model\n",
    "num_heads = 4  # Number of attention heads\n",
    "d_k = d_model // num_heads  # Dimensionality per head\n",
    "group_size = 2  # Number of heads in each group (for GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot-product attention\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "    output = tf.matmul(attention_weights, v)  # [batch_size, num_heads, seq_len_q, depth_v]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Headed Attention (MHA)\n",
    "- In MHA, each head has its own independent query, key, and value projections. \n",
    "    - This allows each head to attend to the input sequence in its own unique way, giving the model more flexibility to learn different types of relationships between tokens.\n",
    "- Since every head can form its own independent attention scores and output, the manifold of possible representations is higher-dimensional. \n",
    "    - This is because the model can learn different attention patterns for each head, covering a wide variety of input-output mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multi-Head Attention (MHA): Independent key-value pairs per head\n",
    "class MHA(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MHA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Key, value, and query layers (unique per head)\n",
    "        self.w_q = layers.Dense(d_model)\n",
    "        self.w_k = layers.Dense(d_model)\n",
    "        self.w_v = layers.Dense(d_model)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [batch_size, num_heads, seq_len, d_k]\n",
    "    \n",
    "    def call(self, query, key, value):\n",
    "        print('=================MHA==================')\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Compute query, key, value projections independently for each head\n",
    "        Q = self.split_heads(self.w_q(query), batch_size)\n",
    "        K = self.split_heads(self.w_k(key), batch_size)\n",
    "        V = self.split_heads(self.w_v(value), batch_size)\n",
    "        \n",
    "        # Print shapes (Optional, for checking differences)\n",
    "        print(f\"MHA Query Shape (Q): {Q.shape}\")\n",
    "        print(f\"{style.RED}MHA Key Shape (K): {K.shape}\")\n",
    "        print(f\"MHA Value Shape (V): {V.shape}\"+ style.RESET)\n",
    "        \n",
    "        attention_output = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Concatenate attention output\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.num_heads * self.d_k))\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-Querry Attention (GQA)\n",
    "- In GQA, multiple heads share key-value pairs within groups, meaning that the attention mechanism is constrained by these shared parameters. \n",
    "    - The heads within a group will compute attention scores based on the same key-value projections, although each head can still have its own query projections.\n",
    "- This reduces the degrees of freedom for the model because the number of independent key-value pairs is smaller than in MHA. \n",
    "    - As a result, the possible space of attention outputs (the manifold) is lower-dimensional compared to MHA, because fewer independent attention computations are possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grouped-Query Attention (GQA)\n",
    "class GQA(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, group_size):\n",
    "        super(GQA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.group_size = group_size\n",
    "        self.num_groups = num_heads // group_size\n",
    "        \n",
    "        # Shared key and value layers per group\n",
    "        self.w_k = layers.Dense(d_model)\n",
    "        self.w_v = layers.Dense(d_model)\n",
    "        \n",
    "        # Query layer (unique per head)\n",
    "        self.w_q = layers.Dense(d_model)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [batch_size, num_heads, seq_len, d_k]\n",
    "    \n",
    "    def call(self, query, key, value):\n",
    "        print('=================GQA==================')\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Compute query projections\n",
    "        Q = self.split_heads(self.w_q(query), batch_size)\n",
    "        print(f\"GQA Query Shape (Q): {Q.shape}\")\n",
    "        \n",
    "        # Compute key and value projections per group\n",
    "        K = self.split_heads(self.w_k(key), batch_size)[:, :self.num_groups, :, :]\n",
    "        V = self.split_heads(self.w_v(value), batch_size)[:, :self.num_groups, :, :]\n",
    "        print(f\"{style.GREEN}GQA Key Shape (K): {K.shape}\")\n",
    "        print(f\"GQA Value Shape (V): {V.shape}\" + style.RESET)\n",
    "        \n",
    "        outputs = []\n",
    "        print('----------------Groups--------------')\n",
    "        for i in range(self.num_groups):\n",
    "            Q_group = Q[:, i * self.group_size:(i + 1) * self.group_size, :, :]\n",
    "            K_group = K[:, i:i+1, :, :]  # Shared key within the group\n",
    "            V_group = V[:, i:i+1, :, :]  # Shared value within the group\n",
    "            print(f\"GQA Group {i+1} Query Shape: {Q_group.shape}, Key Shape: {K_group.shape}, Value Shape: {V_group.shape}\")\n",
    "            group_output = scaled_dot_product_attention(Q_group, K_group, V_group)\n",
    "            outputs.append(group_output)\n",
    "        \n",
    "        # Concatenate outputs from all groups\n",
    "        concat_attention = tf.concat(outputs, axis=1)\n",
    "        concat_attention = tf.transpose(concat_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(concat_attention, (batch_size, -1, self.num_heads * self.d_k))\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.dense(concat_attention)\n",
    "        print('------------------------------------')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Querry Attetion (MQA)\n",
    "\n",
    "- Reduces all key and value heads to a single key and value head\n",
    "- if you have H query, key, and value heads then this will effectively reduce the size of the key-value cache and therefore amount of data that needs to be loaded by a factor of H\n",
    "- MQA can lead to quality degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQA(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MQA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Shared key and value layers\n",
    "        self.w_k = layers.Dense(d_model)\n",
    "        self.w_v = layers.Dense(d_model)\n",
    "        \n",
    "        # Query layer (unique per head)\n",
    "        self.w_q = layers.Dense(d_model)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_k))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [batch_size, num_heads, seq_len, d_k]\n",
    "    \n",
    "    def call(self, query, key, value):\n",
    "        print('=================MQA==================')\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Compute query, key, value projections\n",
    "        Q = self.split_heads(self.w_q(query), batch_size)\n",
    "        K = self.split_heads(self.w_k(key), batch_size)[:, :1, :, :]  # Shared key across all heads\n",
    "        V = self.split_heads(self.w_v(value), batch_size)[:, :1, :, :]  # Shared value across all heads\n",
    "        \n",
    "        # Print shapes of query, key, and value\n",
    "        print(f\"MQA Query Shape (Q): {Q.shape}\")\n",
    "        print(f\"{style.BLUE}MQA Key Shape (K): {K.shape}\" +style.RESET)\n",
    "        print(f\"{style.BLUE}MQA Value Shape (V): {V.shape}\"+ style.RESET)\n",
    "        \n",
    "        # Repeat K and V across heads for multi-query attention\n",
    "        K = tf.tile(K, [1, self.num_heads, 1, 1])\n",
    "        V = tf.tile(V, [1, self.num_heads, 1, 1])\n",
    "        \n",
    "        attention_output = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Concatenate attention output\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.num_heads * self.d_k))\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.dense(concat_attention)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both attention mechanisms\n",
    "input_query = tf.random.normal((batch_size, seq_len, d_model))\n",
    "input_key = tf.random.normal((batch_size, seq_len, d_model))\n",
    "input_value = tf.random.normal((batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MQA and GQA models\n",
    "mha_layer = MHA(d_model, num_heads)\n",
    "gqa_layer = GQA(d_model, num_heads, group_size)\n",
    "mqa_layer = MQA(d_model, num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "### We can see bellow that MHA has the highest memory usage with the longest run-time and MQA has the highest for both. GQA strikes a middle ground between them, allowing for more efficient inference. \n",
    "\n",
    "### Note the shapes of Key and Value are [batch_size, num_groups, seq_len, d_k]. We can see that for K and V of GQA the dimensionality of num_groups = num_heads/group_size = 4/2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================MHA==================\n",
      "MHA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[31mMHA Key Shape (K): (8, 4, 5, 4)\n",
      "MHA Value Shape (V): (8, 4, 5, 4)\u001b[0m\n",
      "Time taken by MHA: 0.023144 seconds\n",
      "Current memory usage: 2.301817 MB; Peak: 2.775021 MB\n",
      "\n",
      "=================GQA==================\n",
      "GQA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[32mGQA Key Shape (K): (8, 2, 5, 4)\n",
      "GQA Value Shape (V): (8, 2, 5, 4)\u001b[0m\n",
      "----------------Groups--------------\n",
      "GQA Group 1 Query Shape: (8, 2, 5, 4), Key Shape: (8, 1, 5, 4), Value Shape: (8, 1, 5, 4)\n",
      "GQA Group 2 Query Shape: (8, 2, 5, 4), Key Shape: (8, 1, 5, 4), Value Shape: (8, 1, 5, 4)\n",
      "------------------------------------\n",
      "Time taken by GQA: 0.037420 seconds\n",
      "Current memory usage: 0.030244 MB; Peak: 0.039383 MB\n",
      "\n",
      "=================MQA==================\n",
      "MQA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[34mMQA Key Shape (K): (8, 1, 5, 4)\u001b[0m\n",
      "\u001b[34mMQA Value Shape (V): (8, 1, 5, 4)\u001b[0m\n",
      "Time taken by MQA: 0.023154 seconds\n",
      "Current memory usage: 0.025914 MB; Peak: 0.034013 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to track memory usage and compute time\n",
    "def track_memory_and_time(layer, query, key, value, name):\n",
    "    tracemalloc.start()  # Start tracking memory\n",
    "    start_time = time.time()  # Start tracking time\n",
    "    layer_output = layer(query, key, value)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()  # Get memory usage\n",
    "    tracemalloc.stop()\n",
    "    #print(f\"{name} Output Shape: {layer_output.shape}\")\n",
    "    print(f\"Time taken by {name}: {elapsed_time:.6f} seconds\")\n",
    "    print(f\"Current memory usage: {current / 10**6:.6f} MB; Peak: {peak / 10**6:.6f} MB\\n\")\n",
    "\n",
    "# Track memory and compute time for MHA, MQA, and GQA\n",
    "track_memory_and_time(mha_layer, input_query, input_key, input_value, \"MHA\")\n",
    "track_memory_and_time(gqa_layer, input_query, input_key, input_value, \"GQA\")\n",
    "track_memory_and_time(mqa_layer, input_query, input_key, input_value, \"MQA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================MHA==================\n",
      "MHA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[31mMHA Key Shape (K): (8, 4, 5, 4)\n",
      "MHA Value Shape (V): (8, 4, 5, 4)\u001b[0m\n",
      "=================GQA==================\n",
      "GQA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[32mGQA Key Shape (K): (8, 2, 5, 4)\n",
      "GQA Value Shape (V): (8, 2, 5, 4)\u001b[0m\n",
      "----------------Groups--------------\n",
      "GQA Group 1 Query Shape: (8, 2, 5, 4), Key Shape: (8, 1, 5, 4), Value Shape: (8, 1, 5, 4)\n",
      "GQA Group 2 Query Shape: (8, 2, 5, 4), Key Shape: (8, 1, 5, 4), Value Shape: (8, 1, 5, 4)\n",
      "------------------------------------\n",
      "=================MQA==================\n",
      "MQA Query Shape (Q): (8, 4, 5, 4)\n",
      "\u001b[34mMQA Key Shape (K): (8, 1, 5, 4)\u001b[0m\n",
      "\u001b[34mMQA Value Shape (V): (8, 1, 5, 4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Compute outputs for MHA, GQA, and MQA\n",
    "# Measure compute time for MHA\n",
    "\n",
    "start_time = time.time()\n",
    "mha_output = mha_layer(input_query, input_key, input_value)\n",
    "mha_time = time.time() - start_time\n",
    "\n",
    "# Measure compute time for GQA\n",
    "start_time = time.time()\n",
    "gqa_output = gqa_layer(input_query, input_key, input_value)\n",
    "gqa_time = time.time() - start_time\n",
    "\n",
    "# Measure compute time for MQA\n",
    "start_time = time.time()\n",
    "mqa_output = mqa_layer(input_query, input_key, input_value)\n",
    "mqa_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA Output Shape: (8, 5, 16)\n",
      "GQA Output Shape: (8, 5, 16)\n",
      "MQA Output Shape: (8, 5, 16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"MHA Output Shape: {mha_output.shape}\")\n",
    "print(f\"GQA Output Shape: {gqa_output.shape}\")\n",
    "print(f\"MQA Output Shape: {mqa_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Process Time ===========\n",
      "MHA: 0.009213 seconds\n",
      "GQA: 0.013903 seconds\n",
      "MQA: 0.005585 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"========= Process Time ===========\")\n",
    "print(f\"MHA: {mha_time:.6f} seconds\")\n",
    "print(f\"GQA: {gqa_time:.6f} seconds\")\n",
    "print(f\"MQA: {mqa_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
